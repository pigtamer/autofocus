import matplotlib.pyplot as plt
import mxnet as mx
from symbols import resnet50
from mxnet.gluon import nn
from mxnet import nd, sym, autograd, image
from skimage.measure import label as sklbl, regionprops as skrp
from scipy.ndimage import gaussian_filter
import cv2 as cv, numpy as np
from utils.vott_parse import cropToROI


def calcConnect(fmap, gau_sigma=3, thres_ratio=0.8, IF_ABS=False):
    """
    function for region connectivity test
    :param fmap: numpy array. feature map generated by focus branch
    :param gau_sigma: sigma for gaussian smoothing filter
    :param thres_ratio: thresholding ratio relative to the maximum of fmap
    :param IF_ABS: bool, whether to get the abs of input fmap
    :return: tuple (lres, lbl), lres is list of bboxes, lbl is labelled by func
    """
    if IF_ABS:
        fmap = np.abs(fmap)
    fmap = gaussian_filter(fmap, sigma=gau_sigma)
    fmap = (fmap > thres_ratio * fmap.max())
    lbl = sklbl(fmap, connectivity=1)
    props = skrp(lbl)
    lres = []
    for item in props:
        lres.append(item.bbox)
    return (lres, lbl)


def genChip(frame, connect_props, dst_size):
    # TODO:
    #   implement post-process of focus chip generation

    conn_bboxes, conn_lbls = connect_props
    Wf, Hf = frame.shape[-1], frame.shape[-2]
    wf, hf = conn_lbls.shape[-1], conn_lbls.shape[-2]
    l_chips, l_new_loc = [None] * len(conn_bboxes), [None] * len(conn_bboxes)
    for k in range(len(conn_bboxes)):
        x1, y1, x2, y2 = conn_bboxes[k]
        bbox = np.array(conn_bboxes[k])
        bbox[0] = int(Wf / wf) * y1  # omg the transposing of roi...
        bbox[1] = int(Hf / hf) * x1
        bbox[2] = int(Wf / wf) * y2
        bbox[3] = int(Hf / hf) * x2
        print(bbox)
        l_chips[k], l_new_loc[k] = cropToROI(img=frame, img_size_y_x=(Hf, Wf),
                                             roi=bbox, dst_size=dst_size)

    return (l_chips, l_new_loc)


def focustest(fname, isize, net, thres=0, IF_ABS=False):
    frame = cv.imread(fname)
    img = nd.array(frame)
    feature = image.imresize(img, isize, isize).astype('float32')
    X = feature.transpose((2, 0, 1)).expand_dims(axis=0)
    if IF_ABS:
        res = nd.abs(net(X.as_in_context(mx.gpu())))
    else:
        res = net(X.as_in_context(mx.gpu()))
    res *= (res > thres)
    plt.subplot(121)
    plt.imshow(res.asnumpy()[0, 0, :, :]);
    plt.title("Focus Pixels")
    plt.subplot(122)
    plt.imshow(cv.resize(res.asnumpy()[0, 0, :, :], (isize, isize))
               * nd.sum(X[0, :, :, :], axis=0).asnumpy() / 3, cmap='gray');
    plt.title("Heatmap")
    plt.show()


class FocusBranch(nn.Block):
    def __init__(self, **kwargs):
        super(FocusBranch, self).__init__()
        self.net = nn.Sequential()
        self.net.add(
            nn.Conv2D(channels=256, kernel_size=3, strides=1, padding=1),
            nn.Activation(activation='sigmoid'),
            nn.Conv2D(channels=256, kernel_size=3, strides=1, padding=1),
            nn.Activation(activation='sigmoid'),
            nn.MaxPool2D(),
            # nn.Conv2D(channels=256, kernel_size=1, strides=1, padding=0),
            # nn.Activation(activation='relu'),
            nn.Conv2D(channels=1, kernel_size=1, strides=1, padding=0)
        )

    def forward(self, x, **kwargs):
        x = self.net(x)
        # x = nd.softmax(x, axis=1)
        # res = genChip(x)
        return x


class HybridFocusBranch(nn.HybridBlock):
    def __init__(self, **kwargs):
        super().__init__()
        self.net = nn.HybridSequential()
        self.net.add(
            nn.Conv2D(channels=256, kernel_size=3, strides=1, padding=1),
            nn.Activation(activation='relu'),
            nn.Conv2D(channels=256, kernel_size=1, strides=1, padding=0),
            nn.Activation(activation='relu'),
            nn.Conv2D(channels=256, kernel_size=1, strides=1, padding=0),
            nn.Activation(activation='tanh'),
            nn.Conv2D(channels=1, kernel_size=1, strides=1, padding=0)
        )

    def hybrid_forward(self, F, x, *args, **kwargs):
        x = self.net(x)
        # x = sym.softmax(x, axis=1)
        # res = genChip(x)
        return x


def test():
    x = nd.random.normal(0, 1, (1, 3, 512, 512), ctx=mx.gpu())
    basenet = resnet50.ResNet50(params=resnet50.params, IF_DENSE=False)
    focusnet = nn.Sequential()
    focusnet.add(
        basenet,
        FocusBranch()
    )
    focusnet.initialize(ctx=mx.gpu())
    x = focusnet(x)
